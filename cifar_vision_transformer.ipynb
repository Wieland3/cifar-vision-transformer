{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nNp4lYiKkGh"
      },
      "source": [
        "## cifar-vision-transformer\n",
        "\n",
        "Implementation of a Vision Transformer from scratch in Keras, following the Keras code example \"Image classification with Vision Transformer\"\n",
        "\n",
        "The original code can be found here:\n",
        "\n",
        "https://keras.io/examples/vision/image_classification_with_vision_transformer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci0GH3BnJ1Ux"
      },
      "source": [
        "## Download and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-VSnQmdPlvw"
      },
      "outputs": [],
      "source": [
        "pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tktbGztPPZ1Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2sL2CIAAKo9"
      },
      "source": [
        "## Transformer Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vhSVUnZp1Kig"
      },
      "outputs": [],
      "source": [
        "PROJECTION_DIM = 108\n",
        "NUM_HEADS = 6\n",
        "TRANSFORMER_UNITS = [\n",
        "    PROJECTION_DIM * 2,\n",
        "    PROJECTION_DIM,\n",
        "]\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlD9RY--ANvc"
      },
      "source": [
        "## Data Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RSIobpdAPgJ0"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 100\n",
        "INPUT_SHAPE = (32, 32, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7so5eY2J5wF"
      },
      "source": [
        "## Download Cifar 100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ndDwwkAQOB",
        "outputId": "563101f8-8b76-478e-a136-35fd2db63957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 14s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZny1B1tJ8H6"
      },
      "source": [
        "## Patch Creation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ar05Bwz5PiOQ"
      },
      "outputs": [],
      "source": [
        "class Patches(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EgJJhMtKAFj"
      },
      "source": [
        "## Patch Encoding Layer\n",
        "\n",
        "Embeds the individual Patches and adds positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DYiHgXWdQmnu"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = tf.keras.layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH0wHuqPKCth"
      },
      "source": [
        "## Multilayer Perceptron Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gE5DZPoe2-4c"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = tf.keras.layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIKUGMWCKJ25"
      },
      "source": [
        "## Building the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_M8U8Xfg418E"
      },
      "outputs": [],
      "source": [
        "def create_vision_transformer():\n",
        "\n",
        "  i = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
        "\n",
        "  patches = Patches(6)(i)\n",
        "  encoded = PatchEncoder(25, PROJECTION_DIM)(patches)\n",
        "\n",
        "  for _ in range(2):\n",
        "\n",
        "    norm_0 = tf.keras.layers.LayerNormalization()(encoded)\n",
        "    attention = tf.keras.layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim = PROJECTION_DIM)(norm_0, norm_0)\n",
        "\n",
        "    skip = tf.keras.layers.Add()([attention, encoded])\n",
        "    norm_1 = tf.keras.layers.LayerNormalization()(skip)\n",
        "\n",
        "    perceptron_layer = mlp(norm_1, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
        "    encoded = tf.keras.layers.Add()([norm_1, perceptron_layer])\n",
        "\n",
        "  norm_2 = tf.keras.layers.LayerNormalization()(encoded)\n",
        "  flat = tf.keras.layers.Flatten()(norm_2)\n",
        "  drop = tf.keras.layers.Dropout(0.5)(flat)\n",
        "\n",
        "  features = mlp(drop, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
        "  logits = tf.keras.layers.Dense(NUM_CLASSES)(features)\n",
        "\n",
        "  return tf.keras.Model(inputs=i, outputs=logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IBY7Tiji6581"
      },
      "outputs": [],
      "source": [
        "transformer = create_vision_transformer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSwSvZ7S69WN",
        "outputId": "69d2edfd-88cf-4f3d-9f4c-bc7fd045e2b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " patches (Patches)           (None, None, 108)            0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncode  (None, 25, 108)              14472     ['patches[0][0]']             \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 25, 108)              216       ['patch_encoder[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 25, 108)              281988    ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 25, 108)              0         ['multi_head_attention[0][0]',\n",
            "                                                                     'patch_encoder[0][0]']       \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 25, 108)              216       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 25, 216)              23544     ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 25, 216)              0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 25, 108)              23436     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 25, 108)              0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 25, 108)              0         ['layer_normalization_1[0][0]'\n",
            "                                                                    , 'dropout_1[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 25, 108)              216       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 25, 108)              281988    ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 25, 108)              0         ['multi_head_attention_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'add_1[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 25, 108)              216       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 25, 216)              23544     ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 25, 216)              0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 25, 108)              23436     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 25, 108)              0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 25, 108)              0         ['layer_normalization_3[0][0]'\n",
            "                                                                    , 'dropout_3[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 25, 108)              216       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 2700)                 0         ['layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 2700)                 0         ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 216)                  583416    ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 216)                  0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 108)                  23436     ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 108)                  0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 100)                  10900     ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1291240 (4.93 MB)\n",
            "Trainable params: 1291240 (4.93 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzXDON78KQOY"
      },
      "source": [
        "## Training the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GHflN--R7IqE"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs=100):\n",
        "\n",
        "  optimizer = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "  model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "  model.fit(x_train, y_train, epochs=epochs, validation_split=0.1)\n",
        "  _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "  print(\"Model Accuracy: \", accuracy)\n",
        "  print(\"Model top-5 Accuracy: \", top_5_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juHbeUjD-YzH",
        "outputId": "f18cef11-048d-4008-f332-976a65ce4ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 27s 13ms/step - loss: 4.6144 - accuracy: 0.0100 - top-5-accuracy: 0.0473 - val_loss: 4.6063 - val_accuracy: 0.0102 - val_top-5-accuracy: 0.0480\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 4.5183 - accuracy: 0.0188 - top-5-accuracy: 0.0834 - val_loss: 4.2990 - val_accuracy: 0.0372 - val_top-5-accuracy: 0.1724\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 4.0765 - accuracy: 0.0639 - top-5-accuracy: 0.2321 - val_loss: 3.9018 - val_accuracy: 0.0890 - val_top-5-accuracy: 0.2966\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 3.7101 - accuracy: 0.1209 - top-5-accuracy: 0.3534 - val_loss: 3.5297 - val_accuracy: 0.1542 - val_top-5-accuracy: 0.4032\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 18s 12ms/step - loss: 3.4802 - accuracy: 0.1592 - top-5-accuracy: 0.4254 - val_loss: 3.3257 - val_accuracy: 0.1924 - val_top-5-accuracy: 0.4654\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 3.3355 - accuracy: 0.1876 - top-5-accuracy: 0.4634 - val_loss: 3.1913 - val_accuracy: 0.2170 - val_top-5-accuracy: 0.5028\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 3.2219 - accuracy: 0.2107 - top-5-accuracy: 0.4942 - val_loss: 3.1187 - val_accuracy: 0.2278 - val_top-5-accuracy: 0.5188\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 3.1421 - accuracy: 0.2260 - top-5-accuracy: 0.5166 - val_loss: 3.0375 - val_accuracy: 0.2526 - val_top-5-accuracy: 0.5386\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 16s 12ms/step - loss: 3.0682 - accuracy: 0.2397 - top-5-accuracy: 0.5363 - val_loss: 3.0103 - val_accuracy: 0.2548 - val_top-5-accuracy: 0.5456\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 16s 12ms/step - loss: 2.9998 - accuracy: 0.2548 - top-5-accuracy: 0.5517 - val_loss: 2.9578 - val_accuracy: 0.2686 - val_top-5-accuracy: 0.5640\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.9413 - accuracy: 0.2678 - top-5-accuracy: 0.5692 - val_loss: 2.9308 - val_accuracy: 0.2840 - val_top-5-accuracy: 0.5678\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.9002 - accuracy: 0.2757 - top-5-accuracy: 0.5774 - val_loss: 2.9188 - val_accuracy: 0.2774 - val_top-5-accuracy: 0.5716\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 16s 12ms/step - loss: 2.8523 - accuracy: 0.2837 - top-5-accuracy: 0.5888 - val_loss: 2.8545 - val_accuracy: 0.2842 - val_top-5-accuracy: 0.5856\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.8155 - accuracy: 0.2916 - top-5-accuracy: 0.5990 - val_loss: 2.8431 - val_accuracy: 0.2904 - val_top-5-accuracy: 0.5876\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.7774 - accuracy: 0.2987 - top-5-accuracy: 0.6085 - val_loss: 2.8508 - val_accuracy: 0.2928 - val_top-5-accuracy: 0.5856\n",
            "Epoch 16/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.7375 - accuracy: 0.3060 - top-5-accuracy: 0.6166 - val_loss: 2.7853 - val_accuracy: 0.3114 - val_top-5-accuracy: 0.5994\n",
            "Epoch 17/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.7081 - accuracy: 0.3148 - top-5-accuracy: 0.6242 - val_loss: 2.7707 - val_accuracy: 0.3070 - val_top-5-accuracy: 0.6050\n",
            "Epoch 18/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.6739 - accuracy: 0.3190 - top-5-accuracy: 0.6322 - val_loss: 2.7797 - val_accuracy: 0.3072 - val_top-5-accuracy: 0.6038\n",
            "Epoch 19/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.6545 - accuracy: 0.3234 - top-5-accuracy: 0.6379 - val_loss: 2.7342 - val_accuracy: 0.3124 - val_top-5-accuracy: 0.6102\n",
            "Epoch 20/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.6321 - accuracy: 0.3286 - top-5-accuracy: 0.6421 - val_loss: 2.7896 - val_accuracy: 0.3042 - val_top-5-accuracy: 0.6042\n",
            "Epoch 21/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.6220 - accuracy: 0.3317 - top-5-accuracy: 0.6457 - val_loss: 2.7413 - val_accuracy: 0.3176 - val_top-5-accuracy: 0.6144\n",
            "Epoch 22/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.5893 - accuracy: 0.3363 - top-5-accuracy: 0.6509 - val_loss: 2.7264 - val_accuracy: 0.3204 - val_top-5-accuracy: 0.6122\n",
            "Epoch 23/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.5695 - accuracy: 0.3416 - top-5-accuracy: 0.6576 - val_loss: 2.7197 - val_accuracy: 0.3292 - val_top-5-accuracy: 0.6150\n",
            "Epoch 24/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.5502 - accuracy: 0.3438 - top-5-accuracy: 0.6583 - val_loss: 2.7170 - val_accuracy: 0.3244 - val_top-5-accuracy: 0.6174\n",
            "Epoch 25/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.5286 - accuracy: 0.3494 - top-5-accuracy: 0.6681 - val_loss: 2.6648 - val_accuracy: 0.3312 - val_top-5-accuracy: 0.6332\n",
            "Epoch 26/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.5173 - accuracy: 0.3513 - top-5-accuracy: 0.6675 - val_loss: 2.6802 - val_accuracy: 0.3290 - val_top-5-accuracy: 0.6278\n",
            "Epoch 27/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.5023 - accuracy: 0.3576 - top-5-accuracy: 0.6724 - val_loss: 2.6660 - val_accuracy: 0.3308 - val_top-5-accuracy: 0.6288\n",
            "Epoch 28/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.4773 - accuracy: 0.3617 - top-5-accuracy: 0.6811 - val_loss: 2.6869 - val_accuracy: 0.3270 - val_top-5-accuracy: 0.6272\n",
            "Epoch 29/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.4681 - accuracy: 0.3622 - top-5-accuracy: 0.6808 - val_loss: 2.6625 - val_accuracy: 0.3364 - val_top-5-accuracy: 0.6330\n",
            "Epoch 30/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.4541 - accuracy: 0.3648 - top-5-accuracy: 0.6833 - val_loss: 2.6574 - val_accuracy: 0.3402 - val_top-5-accuracy: 0.6294\n",
            "Epoch 31/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.4395 - accuracy: 0.3670 - top-5-accuracy: 0.6872 - val_loss: 2.6308 - val_accuracy: 0.3412 - val_top-5-accuracy: 0.6434\n",
            "Epoch 32/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.4212 - accuracy: 0.3745 - top-5-accuracy: 0.6886 - val_loss: 2.6299 - val_accuracy: 0.3458 - val_top-5-accuracy: 0.6390\n",
            "Epoch 33/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.4153 - accuracy: 0.3734 - top-5-accuracy: 0.6917 - val_loss: 2.6600 - val_accuracy: 0.3336 - val_top-5-accuracy: 0.6362\n",
            "Epoch 34/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.4091 - accuracy: 0.3734 - top-5-accuracy: 0.6932 - val_loss: 2.5888 - val_accuracy: 0.3470 - val_top-5-accuracy: 0.6448\n",
            "Epoch 35/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.3847 - accuracy: 0.3768 - top-5-accuracy: 0.6984 - val_loss: 2.5806 - val_accuracy: 0.3574 - val_top-5-accuracy: 0.6460\n",
            "Epoch 36/100\n",
            "1407/1407 [==============================] - 18s 12ms/step - loss: 2.3816 - accuracy: 0.3828 - top-5-accuracy: 0.6982 - val_loss: 2.6205 - val_accuracy: 0.3420 - val_top-5-accuracy: 0.6400\n",
            "Epoch 37/100\n",
            "1407/1407 [==============================] - 16s 12ms/step - loss: 2.3529 - accuracy: 0.3861 - top-5-accuracy: 0.7045 - val_loss: 2.5820 - val_accuracy: 0.3516 - val_top-5-accuracy: 0.6496\n",
            "Epoch 38/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.3591 - accuracy: 0.3799 - top-5-accuracy: 0.7056 - val_loss: 2.5903 - val_accuracy: 0.3472 - val_top-5-accuracy: 0.6530\n",
            "Epoch 39/100\n",
            "1407/1407 [==============================] - 18s 12ms/step - loss: 2.3446 - accuracy: 0.3867 - top-5-accuracy: 0.7072 - val_loss: 2.5754 - val_accuracy: 0.3544 - val_top-5-accuracy: 0.6524\n",
            "Epoch 40/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.3334 - accuracy: 0.3895 - top-5-accuracy: 0.7097 - val_loss: 2.5890 - val_accuracy: 0.3494 - val_top-5-accuracy: 0.6494\n",
            "Epoch 41/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.3318 - accuracy: 0.3906 - top-5-accuracy: 0.7085 - val_loss: 2.6251 - val_accuracy: 0.3418 - val_top-5-accuracy: 0.6434\n",
            "Epoch 42/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.3228 - accuracy: 0.3921 - top-5-accuracy: 0.7097 - val_loss: 2.5814 - val_accuracy: 0.3488 - val_top-5-accuracy: 0.6478\n",
            "Epoch 43/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.3093 - accuracy: 0.3963 - top-5-accuracy: 0.7150 - val_loss: 2.5602 - val_accuracy: 0.3574 - val_top-5-accuracy: 0.6554\n",
            "Epoch 44/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.3041 - accuracy: 0.3956 - top-5-accuracy: 0.7159 - val_loss: 2.5770 - val_accuracy: 0.3576 - val_top-5-accuracy: 0.6520\n",
            "Epoch 45/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.3035 - accuracy: 0.3957 - top-5-accuracy: 0.7181 - val_loss: 2.5573 - val_accuracy: 0.3542 - val_top-5-accuracy: 0.6600\n",
            "Epoch 46/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2949 - accuracy: 0.3973 - top-5-accuracy: 0.7168 - val_loss: 2.5323 - val_accuracy: 0.3624 - val_top-5-accuracy: 0.6626\n",
            "Epoch 47/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2855 - accuracy: 0.4003 - top-5-accuracy: 0.7208 - val_loss: 2.5185 - val_accuracy: 0.3636 - val_top-5-accuracy: 0.6728\n",
            "Epoch 48/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2734 - accuracy: 0.4004 - top-5-accuracy: 0.7232 - val_loss: 2.5627 - val_accuracy: 0.3500 - val_top-5-accuracy: 0.6570\n",
            "Epoch 49/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2761 - accuracy: 0.4009 - top-5-accuracy: 0.7240 - val_loss: 2.5595 - val_accuracy: 0.3600 - val_top-5-accuracy: 0.6552\n",
            "Epoch 50/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2514 - accuracy: 0.4076 - top-5-accuracy: 0.7286 - val_loss: 2.5294 - val_accuracy: 0.3600 - val_top-5-accuracy: 0.6644\n",
            "Epoch 51/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.2558 - accuracy: 0.4062 - top-5-accuracy: 0.7294 - val_loss: 2.5291 - val_accuracy: 0.3676 - val_top-5-accuracy: 0.6602\n",
            "Epoch 52/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2475 - accuracy: 0.4099 - top-5-accuracy: 0.7271 - val_loss: 2.5227 - val_accuracy: 0.3602 - val_top-5-accuracy: 0.6652\n",
            "Epoch 53/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 2.2370 - accuracy: 0.4078 - top-5-accuracy: 0.7278 - val_loss: 2.5240 - val_accuracy: 0.3624 - val_top-5-accuracy: 0.6584\n",
            "Epoch 54/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2323 - accuracy: 0.4087 - top-5-accuracy: 0.7317 - val_loss: 2.4866 - val_accuracy: 0.3644 - val_top-5-accuracy: 0.6704\n",
            "Epoch 55/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2244 - accuracy: 0.4119 - top-5-accuracy: 0.7335 - val_loss: 2.4959 - val_accuracy: 0.3656 - val_top-5-accuracy: 0.6700\n",
            "Epoch 56/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.2126 - accuracy: 0.4124 - top-5-accuracy: 0.7356 - val_loss: 2.4837 - val_accuracy: 0.3802 - val_top-5-accuracy: 0.6728\n",
            "Epoch 57/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.2017 - accuracy: 0.4161 - top-5-accuracy: 0.7384 - val_loss: 2.4566 - val_accuracy: 0.3796 - val_top-5-accuracy: 0.6762\n",
            "Epoch 58/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1891 - accuracy: 0.4174 - top-5-accuracy: 0.7417 - val_loss: 2.4903 - val_accuracy: 0.3698 - val_top-5-accuracy: 0.6658\n",
            "Epoch 59/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.1871 - accuracy: 0.4223 - top-5-accuracy: 0.7424 - val_loss: 2.4480 - val_accuracy: 0.3774 - val_top-5-accuracy: 0.6826\n",
            "Epoch 60/100\n",
            "1407/1407 [==============================] - 16s 12ms/step - loss: 2.1768 - accuracy: 0.4222 - top-5-accuracy: 0.7416 - val_loss: 2.4724 - val_accuracy: 0.3764 - val_top-5-accuracy: 0.6726\n",
            "Epoch 61/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1790 - accuracy: 0.4211 - top-5-accuracy: 0.7407 - val_loss: 2.4208 - val_accuracy: 0.3848 - val_top-5-accuracy: 0.6860\n",
            "Epoch 62/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.1661 - accuracy: 0.4256 - top-5-accuracy: 0.7448 - val_loss: 2.4875 - val_accuracy: 0.3700 - val_top-5-accuracy: 0.6752\n",
            "Epoch 63/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1597 - accuracy: 0.4238 - top-5-accuracy: 0.7478 - val_loss: 2.4630 - val_accuracy: 0.3746 - val_top-5-accuracy: 0.6794\n",
            "Epoch 64/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1555 - accuracy: 0.4261 - top-5-accuracy: 0.7470 - val_loss: 2.4311 - val_accuracy: 0.3874 - val_top-5-accuracy: 0.6808\n",
            "Epoch 65/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.1539 - accuracy: 0.4287 - top-5-accuracy: 0.7481 - val_loss: 2.4252 - val_accuracy: 0.3788 - val_top-5-accuracy: 0.6868\n",
            "Epoch 66/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1470 - accuracy: 0.4294 - top-5-accuracy: 0.7487 - val_loss: 2.4320 - val_accuracy: 0.3776 - val_top-5-accuracy: 0.6892\n",
            "Epoch 67/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1448 - accuracy: 0.4281 - top-5-accuracy: 0.7512 - val_loss: 2.4059 - val_accuracy: 0.3888 - val_top-5-accuracy: 0.6946\n",
            "Epoch 68/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1263 - accuracy: 0.4307 - top-5-accuracy: 0.7536 - val_loss: 2.3959 - val_accuracy: 0.3862 - val_top-5-accuracy: 0.6956\n",
            "Epoch 69/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1271 - accuracy: 0.4337 - top-5-accuracy: 0.7500 - val_loss: 2.4177 - val_accuracy: 0.3828 - val_top-5-accuracy: 0.6854\n",
            "Epoch 70/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1209 - accuracy: 0.4344 - top-5-accuracy: 0.7554 - val_loss: 2.4109 - val_accuracy: 0.3958 - val_top-5-accuracy: 0.6870\n",
            "Epoch 71/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.1206 - accuracy: 0.4349 - top-5-accuracy: 0.7539 - val_loss: 2.4260 - val_accuracy: 0.3912 - val_top-5-accuracy: 0.6892\n",
            "Epoch 72/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1165 - accuracy: 0.4348 - top-5-accuracy: 0.7559 - val_loss: 2.4297 - val_accuracy: 0.3770 - val_top-5-accuracy: 0.6806\n",
            "Epoch 73/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1079 - accuracy: 0.4360 - top-5-accuracy: 0.7580 - val_loss: 2.3806 - val_accuracy: 0.3890 - val_top-5-accuracy: 0.6974\n",
            "Epoch 74/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.1044 - accuracy: 0.4375 - top-5-accuracy: 0.7592 - val_loss: 2.3987 - val_accuracy: 0.3864 - val_top-5-accuracy: 0.6866\n",
            "Epoch 75/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.1007 - accuracy: 0.4387 - top-5-accuracy: 0.7586 - val_loss: 2.3692 - val_accuracy: 0.3962 - val_top-5-accuracy: 0.7030\n",
            "Epoch 76/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.1081 - accuracy: 0.4368 - top-5-accuracy: 0.7550 - val_loss: 2.3693 - val_accuracy: 0.3966 - val_top-5-accuracy: 0.6972\n",
            "Epoch 77/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0916 - accuracy: 0.4384 - top-5-accuracy: 0.7612 - val_loss: 2.3723 - val_accuracy: 0.3860 - val_top-5-accuracy: 0.7046\n",
            "Epoch 78/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0838 - accuracy: 0.4402 - top-5-accuracy: 0.7597 - val_loss: 2.4095 - val_accuracy: 0.3816 - val_top-5-accuracy: 0.6964\n",
            "Epoch 79/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0893 - accuracy: 0.4423 - top-5-accuracy: 0.7603 - val_loss: 2.3524 - val_accuracy: 0.3944 - val_top-5-accuracy: 0.7016\n",
            "Epoch 80/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0889 - accuracy: 0.4412 - top-5-accuracy: 0.7608 - val_loss: 2.3669 - val_accuracy: 0.3968 - val_top-5-accuracy: 0.6994\n",
            "Epoch 81/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0807 - accuracy: 0.4418 - top-5-accuracy: 0.7615 - val_loss: 2.4103 - val_accuracy: 0.3916 - val_top-5-accuracy: 0.6952\n",
            "Epoch 82/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0878 - accuracy: 0.4401 - top-5-accuracy: 0.7633 - val_loss: 2.3518 - val_accuracy: 0.3996 - val_top-5-accuracy: 0.7002\n",
            "Epoch 83/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0689 - accuracy: 0.4443 - top-5-accuracy: 0.7637 - val_loss: 2.3531 - val_accuracy: 0.3998 - val_top-5-accuracy: 0.6972\n",
            "Epoch 84/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0647 - accuracy: 0.4469 - top-5-accuracy: 0.7655 - val_loss: 2.3586 - val_accuracy: 0.3944 - val_top-5-accuracy: 0.7014\n",
            "Epoch 85/100\n",
            "1407/1407 [==============================] - 18s 12ms/step - loss: 2.0643 - accuracy: 0.4438 - top-5-accuracy: 0.7658 - val_loss: 2.4553 - val_accuracy: 0.3762 - val_top-5-accuracy: 0.6778\n",
            "Epoch 86/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0737 - accuracy: 0.4437 - top-5-accuracy: 0.7624 - val_loss: 2.3363 - val_accuracy: 0.4062 - val_top-5-accuracy: 0.7040\n",
            "Epoch 87/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0715 - accuracy: 0.4463 - top-5-accuracy: 0.7656 - val_loss: 2.3404 - val_accuracy: 0.4018 - val_top-5-accuracy: 0.7064\n",
            "Epoch 88/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0589 - accuracy: 0.4483 - top-5-accuracy: 0.7674 - val_loss: 2.3287 - val_accuracy: 0.4080 - val_top-5-accuracy: 0.7022\n",
            "Epoch 89/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0598 - accuracy: 0.4468 - top-5-accuracy: 0.7654 - val_loss: 2.3683 - val_accuracy: 0.3984 - val_top-5-accuracy: 0.6988\n",
            "Epoch 90/100\n",
            "1407/1407 [==============================] - 18s 12ms/step - loss: 2.0640 - accuracy: 0.4450 - top-5-accuracy: 0.7652 - val_loss: 2.4108 - val_accuracy: 0.3888 - val_top-5-accuracy: 0.6942\n",
            "Epoch 91/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0662 - accuracy: 0.4466 - top-5-accuracy: 0.7667 - val_loss: 2.3721 - val_accuracy: 0.3926 - val_top-5-accuracy: 0.7052\n",
            "Epoch 92/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0622 - accuracy: 0.4477 - top-5-accuracy: 0.7688 - val_loss: 2.3115 - val_accuracy: 0.4086 - val_top-5-accuracy: 0.7130\n",
            "Epoch 93/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0524 - accuracy: 0.4498 - top-5-accuracy: 0.7675 - val_loss: 2.3357 - val_accuracy: 0.4064 - val_top-5-accuracy: 0.7016\n",
            "Epoch 94/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0401 - accuracy: 0.4485 - top-5-accuracy: 0.7718 - val_loss: 2.3538 - val_accuracy: 0.3954 - val_top-5-accuracy: 0.7020\n",
            "Epoch 95/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.0431 - accuracy: 0.4510 - top-5-accuracy: 0.7690 - val_loss: 2.3302 - val_accuracy: 0.4012 - val_top-5-accuracy: 0.7030\n",
            "Epoch 96/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 2.0421 - accuracy: 0.4511 - top-5-accuracy: 0.7694 - val_loss: 2.3171 - val_accuracy: 0.4058 - val_top-5-accuracy: 0.7040\n",
            "Epoch 97/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0460 - accuracy: 0.4516 - top-5-accuracy: 0.7682 - val_loss: 2.3751 - val_accuracy: 0.3964 - val_top-5-accuracy: 0.6918\n",
            "Epoch 98/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0406 - accuracy: 0.4496 - top-5-accuracy: 0.7698 - val_loss: 2.3350 - val_accuracy: 0.4038 - val_top-5-accuracy: 0.7038\n",
            "Epoch 99/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0367 - accuracy: 0.4505 - top-5-accuracy: 0.7722 - val_loss: 2.3906 - val_accuracy: 0.3982 - val_top-5-accuracy: 0.6942\n",
            "Epoch 100/100\n",
            "1407/1407 [==============================] - 17s 12ms/step - loss: 2.0391 - accuracy: 0.4510 - top-5-accuracy: 0.7705 - val_loss: 2.3182 - val_accuracy: 0.4058 - val_top-5-accuracy: 0.7078\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 2.2738 - accuracy: 0.4097 - top-5-accuracy: 0.7145\n",
            "Model Accuracy:  0.4097000062465668\n",
            "Model top-5 Accuracy:  0.7145000100135803\n"
          ]
        }
      ],
      "source": [
        "train(transformer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
